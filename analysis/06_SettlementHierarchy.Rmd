---
title: "Settlement Persistence Project, SBOM Script #6:"
subtitle: "Global and Local Settlement Hierarchy"
author: "Rudolf Cesaretti"
date: "Last run on `r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
    number_sections: true
bibliography: References.bib
csl: apa.csl
link-citations: yes
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r, setup, include=FALSE,echo=FALSE, message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=75),tidy=TRUE)
#
rm(list = ls())
```

I do four things in this R markdown document: 

  3. Calculate settlement hierarchy metrics for AggSites at two spatial scales:
      + The global level (SBOM-region-wide) 
      + The local level (at a set radius surrounding each site)
  4. Reorganize the data and export for Script #7
  
  
# Setup 

All of the data and scripts are downloadable from the [new ASU SettlementPersist2022 github repository](https://https://github.com/rcesaret/ASUSettlementPersist2022), which can be downloaded locally as a .zip folder or cloned to your own account.

Either way, once you have done so, you will need to modify the working directory (setwd("C:/...)") path and "dir" variables in the code chunk below to match the repository location on your computer.

```{r, label='Set Local Directory Location', message=FALSE,warning=FALSE}

wd <- list()

#SET YOUR LOCAL DIRECTORY LOCATION HERE:
wd$dir <- "C:/Users/rcesaret/Dropbox (ASU)/ASUSettlementPersist2022/"
#wd$dir <- "C:/Users/TJ McMote/Dropbox (ASU)/ASUSettlementPersist2022"

wd$analysis <- paste0(wd$dir,"analysis/")
wd$data_r <- paste0(wd$dir,"data-raw/")
wd$data_p <- paste0(wd$dir,"data-processed/")
wd$data_f <- paste0(wd$dir,"data-final-outputs/")
wd$figs <- paste0(wd$dir,"figures/")
wd$funcs <- paste0(wd$dir,"functions/")

```


## Load R Packages and Custom Functions

```{r, label='Load Libraries', message=FALSE,warning=FALSE}
# Package names
packages <- c("rgdal", "rgeos", "sp", "sf", "GISTools", "raster", "Matrix", "gdistance", "lwgeom", "tidyverse", "tidyr", "classInt", "pacman", "RColorBrewer", "cowplot", "stars", "ggnewscale")#, "data.table", "zoo", "mgcv","igraph", "ggrepel","ggridges", "movecost",  "datplot", "scales",

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# load packages
invisible(lapply(packages, library, character.only = TRUE))

rm(packages,installed_packages)

#Read in custom R functions located in the wd$funcs directory folder
FUNCS <- list("splitByAttributes.R", "RS_Acoef.R", "SettHierClass_ggplot.R")
invisible(lapply(FUNCS, function(x) source(paste0(wd$funcs,x))))
rm(FUNCS)

```


## Import Data

Data we are importing:

  1. the AggSite polygon data
  2. A simple polygon calculated in QGIS that specifies a hard outter border for the catchment areas of sites (constructed for sensitivity to survey borders and sites not included in the SBOM sample)
  3. Cost-distance matrices from script #3
  4. Least cost path rasters from script #3
  5. A raster hillshade basemap for the SBOM which includes the lakes

```{r, label='Import Data', message=FALSE,warning=FALSE}

#Agg Site and catchment polygon data
All_AggPoly <- readOGR(paste0(wd$data_p,"SBOM_AggSitePoly5.gpkg"))
All_CatchPoly <- readOGR(paste0(wd$data_p,"SBOM_CatchPoly5.gpkg"))

#Catchment boundary limit polygon
CatchLims <- readOGR(paste0(wd$data_r,"CatchLims.gpkg"))
CatchLims.sf = st_as_sf(CatchLims) #for ggplot 

## Hillshade Basemap Raster with lake
HillshadeLake <- raster(paste0(wd$data_r, "HillshadeLake.tif"))
HillshadeLake <- rast(HillshadeLake, crs = 26914)
Hillshade.s <- st_as_stars(HillshadeLake) #for ggplot basemap

#Cost-distance matrices
ord <- c("EF", "EF_MF", "MF", "MF_LF", "LF", "LF_TF", "TF", "TF_CL", "CL", "CL_ET", "ET", "ET_LTAzI", "LTAzI", "LTAzI_EA", "EA", "EA_LA", "LA")
temp = list.files(path = paste0(wd$data_p,"CDMatrices/"), full.names = TRUE)
nam.distmat = list.files(path = paste0(wd$data_p,"CDMatrices/"), full.names = F)
nam.distmat = gsub('_cdmat.csv', "", nam.distmat)
CD.mats = lapply(temp, read.csv, header = TRUE, row.names=1)
CD.mats = lapply(CD.mats, as.matrix)
names(CD.mats) <- nam.distmat
CD.mats <- CD.mats[ord]
names(CD.mats) <- paste0(ord,"_cdmat")

```


## Reorganize Data from Step #5

```{r, 'Reorganize Data from Step #5', message=FALSE,warning=FALSE}

#reorder spatial points dataframe by period
All_AggPoly <- All_AggPoly[order(All_AggPoly$PeriodNum),]
All_CatchPoly <- All_CatchPoly[order(All_CatchPoly$PeriodNum),]

# Split polygons by Phase, saved as list of SPDF
Poly_List <- splitByAttributes(spdata = All_AggPoly, attr = "Period", suffix="_SitePoly") 
Catch_List <- splitByAttributes(spdata = All_CatchPoly, attr = "Period", suffix="_CatchPoly") 

# convert spatial polygons dataframe to spatial points dataframe
coor = All_AggPoly@data[,c("East","North")] #create separate dataframe of coordinates
rownames(coor) <- as.numeric(rownames(coor)) #make sure rownames match
All_AggPts <- SpatialPointsDataFrame(coor, All_AggPoly@data, match.ID = TRUE) #convert to points
proj4string(All_AggPts) <- CRS("+proj=utm +zone=14 +datum=NAD83 +units=m +no_defs") #set CRS
All_AggPts = as(st_make_valid(st_as_sf(All_AggPts)), "Spatial") #make sure geometry is valid
All_AggPts <- All_AggPts[order(All_AggPts$PeriodNum),]#reorder by period

# Split points by Phase, saved as list of spatial points dataframes
Pts_List <- splitByAttributes(spdata = All_AggPts, attr = "Period", suffix="_Pts")

```



# Global Settlement Hierarchy Metrics


## Settlement Hierarchy Levels

Classifying the hierarchical levels of settlement systems from archaeological data can be notoriously arbitrary without more contextual information. The main issues are why one set of size breaks is better than another, how many levels to include, and the temptation to ideosyncratic/unsystematic/non-reproducible human choices that vary between periods/cases/regions.

### Number of Classes for Each Period

To get around these issues, I employ a systematic and reproducible two-part methodology. First, the number of hierarchical levels is chosen as the rounded mean of several different established methods of computing the **number of classes** (as opposed to partitioning the class intervals themselves) for a univariate distribution such as settlement population sizes. The methods that performed the best on our right-skew city size distributions are:

  1. **Sturges' formula** where bin sizes are based on the range of the data [@Sturges1926], calculated using the "grDevices::nclass" function [@R2022]
  2. **Scott's method** based on the standard error of a Gaussian distribution fitted to the data [@Scott1992]. To make estimates of this method valid, Log Population was used instead of population. Scott's classes were calculated using the "grDevices::nclass" function [@R2022]
  3. **Jiang's Head/Tail method**, designed for heavy-tailed distributions, partitioning the distribution by the mean until the head-tail ratio is deskewed beyond a threshold level [@Jiang2013]. Jiang's method was calculated using the "classInt" R package [@Bivand2022].
  4. **Manually** via the traditional archaeological method of locating gaps in the distribution through close visual inspection of histograms and ECDFs (and a mind towards contextually-likely settlement hierarchy size classes). These are as follows:
  
```{r, 'Manual Method Settlement Hierarchy Classes', echo=FALSE,message=FALSE,warning=FALSE}
m = All_AggPoly@data %>% 
          group_by(PeriodNum) %>% 
          summarise(
              MinPop = min(Population.s2, na.rm=T),
              MaxPop = max(Population.s2, na.rm=T))

m$brks <- rev(c(", 250, 750, 2000, 4000, 8000, ",
            ", 200, 650, 2000, 10000, 20000, ",
            ", 150, 600, 1250, 3000, 8000, ",
            ", 100, 500, 2000, 7000, 12500, ",
            ", 200, 500, 1250, 2500, 7500, ",
            ", 100, 300, 600, 2000, ",
            ", 125, 350, 1250, 3000, 7500, ",
            ", 300, 2500, ",
            ", 100, 300, 600, 1500, ",
            ", 100, 200, 400, ",
            ", 250, 1000, 2000, 5000, ",
            ", 200, 1000, 2500, 6000, ",
            ", 100, 800, 3500, 7000, ",
            ", 150, 400, 800, ",
            ", 100, 300, 1000, 3000, ",
            ", 120, ",
            ", 200, "))

ManualHierClasses <- data.frame(
        PeriodNum = as.character(c(1:17)),
        Period = c("EF", "EF_MF", "MF", "MF_LF", "LF", "LF_TF", "TF", "TF_CL", "CL", "CL_ET", "ET", "ET_LTAzI", "LTAzI", "LTAzI_EA", "EA", "EA_LA", "LA"),
        Breaks = paste0(round(m$MinPop-1,0),m$brks,round(m$MaxPop+1,0)),
        NumClasses = c(2,2,5,4,5,5,5,4,5,3,6,5,6,6,6,6,6))

knitr::kable(ManualHierClasses, "pipe")

rm(m)
```
  
All four of these were performed on the population data of each period, and the row-wise mean and median (rounded to the nearest integer) were calculated.

```{r, 'Number of Settlement Hierarchy Classes', message=FALSE,warning=FALSE}
Manual <- ManualHierClasses$NumClasses
Sturges <- NA
Scott <- NA
HeadTails <- NA

for (i in 1:length(Poly_List)){
  Sturges[i] <- nclass.Sturges(Poly_List[[i]]$Population.s2)
  Scott[i] <- nclass.scott(Poly_List[[i]]$Log_Population.s2)
  HTbreaks <- classIntervals(Poly_List[[i]]$Population.s2, style = "headtails")
  HeadTails[i] <- length(HTbreaks$brks) - 1
}

levels <- data.frame(PeriodNum=ManualHierClasses$PeriodNum, 
                     Period=ManualHierClasses$Period, 
                     Sturges=Sturges, Scott=Scott, 
                     HeadTails=HeadTails, Manual=Manual)

levels <- levels %>% rowwise() %>% mutate(
  Median = round(median(c(Sturges,HeadTails,Manual)),0),
  Mean = round(mean(c(Sturges,HeadTails,Manual)),0),
) %>% ungroup()

knitr::kable(levels, "pipe" ,align="c")

rm(Manual, Sturges, Scott, HeadTails, HTbreaks)
```
Surprisingly, the _median_ of the four methods is **_very_** close to the "Manual" archaeological reasoning method undertaken by myself -- only differing by 1 for a single period (otherwise identical). The difference between the mean and the median is also quite small. More specifically, the mean is higher than the median by 1 for four periods (otherwise identical).

The Heads/Tails method has a very low sensitivity to such changes overall by design. Because Heads/Tails splits progressively down the distribution by cutting at the mean of the left tail, the increasing number of classes only reflects the magnitude of right-skew (completely insensitive to the number + variability of cases in the left tail). By contrast, Sturges' and Scott's methods are especially responsive to the growing ranges of the distributions, estimating systematically much higher numbers of classes for periods with a large number+range of settlements in a wide-ranging hierarchy (and vice versa). As such, they are both very sensitive to changes in the distribution as a whole. 

In this sense, the mean and median central tendencies nicely balance the extremes of the three systematic methods. The near identity of the mean and median, and their close similarity to the independently estimated "Manual archaeological breaks reasoning" method, both reccomend this middling set of metrics. I think that the mean performs the best because it rounds upwards for periods where there is a high right-skew. Whereas my manual classifications were cautious not to over-split the settlements in the fat right tail, the higher mean seems to indicate that my right tail bin sizes were too wide (i.e. they deserved to be subdivided in some cases, which is being picked up in the mean because of the suystematic shifts in the other metrics). I have therefore chosen to use the mean as the number of hierarchical levels to classify for each period.


### Classification Method

The ["classInt" R Package](https://cran.r-project.org/web/packages/classInt) [@Bivand2022] provides a number of functions for classifying the univariate settlement size (population) hierarchy into the number of groups specified above (i.e. the integer-rounded mean of the four methods). Of these methods, some are not appropriate for a right-skew distribution, and others don't enable us to specify the number of classes. The best performing methods are kmeans, fisher and jenks. All others leave almost the entire fat left tail of the distribution unpartitioned. 

  * kmeans
  * fisher
  * jenks
When the settlement size distribution is not heavily right-skew, all three methods perform about the same. However, with greater degrees of right-skew, Fisher's and Jenks' methods leave most of the left-tail unpartitioned. As seen below, only kmeans provides a partitioning method robust to higher levels of right-skew, creating size classes quite similar to the Manual Archy method. (Note that the intervals are calculated on unlogged data, while the x-axes are logged to make the groups clearer.) As such, Kmeans seems to provide a systematic means of partitioning the settlement distributions.

```{r, 'Hierarchy Classes Example 1', echo=FALSE, message=FALSE,warning=FALSE}
p=15
x=Poly_List[[p]]$Population.s2
nc=levels$Mean[p]
pal1=brewer.pal(nc,"Set2")
fb=as.numeric(unlist(strsplit(ManualHierClasses$Breaks[p], split=", ")))

kmbrks <- classIntervals(x, style = "kmeans", n=nc)
hcbrks <- classIntervals(x, style = "hclust", n=nc)
bcbrks <- classIntervals(x, style = "bclust", n=nc)
fbrks <- classIntervals(x, style = "fisher", n=nc)
jbrks <- classIntervals(x, style = "jenks", n=nc)
maxbrks <- classIntervals(x, style = "maximum", n=nc)
fixbrks <- classIntervals(x, style = "fixed", fixedBreaks=fb)

par(mfrow = c(2, 2))
plot(fbrks, pal = pal1, main = paste0("Fisher, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(jbrks, pal = pal1, main = paste0("Jenks, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(kmbrks, pal = pal1, main = paste0("Kmeans, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(fixbrks, pal = pal1, main = paste0("Manual, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
par(mfrow = c(1, 1))

```

```{r, 'Hierarchy Classes Example 2', echo=FALSE, message=FALSE,warning=FALSE}
p=5
x=Poly_List[[p]]$Population.s2
nc=levels$Mean[p]
pal1=brewer.pal(nc,"Set2")
fb=as.numeric(unlist(strsplit(ManualHierClasses$Breaks[p], split=", ")))

kmbrks <- classIntervals(x, style = "kmeans", n=nc)
hcbrks <- classIntervals(x, style = "hclust", n=nc)
bcbrks <- classIntervals(x, style = "bclust", n=nc)
fbrks <- classIntervals(x, style = "fisher", n=nc)
jbrks <- classIntervals(x, style = "jenks", n=nc)
maxbrks <- classIntervals(x, style = "maximum", n=nc)
fixbrks <- classIntervals(x, style = "fixed", fixedBreaks=fb)

par(mfrow = c(2, 2))
plot(fbrks, pal = pal1, main = paste0("Fisher, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(jbrks, pal = pal1, main = paste0("Jenks, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(kmbrks, pal = pal1, main = paste0("Kmeans, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
plot(fixbrks, pal = pal1, main = paste0("Manual, ",levels$Period[p]," ",levels$PeriodNum[p]),log = 'x',xlim=c(min(x)-1, max(x)+5), xlab="Log Population", ylab="ECDF(Population)")
par(mfrow = c(1, 1))

```

These two periods are also representative in terms of their mixed geographical distribution of size classes across the region, further cross-validating the Kmeans settlement hierarchy class intervals. 

```{r, 'Hierarchy Classes Example Maps', echo=FALSE, message=FALSE,warning=FALSE}

# Settlement Heirarchy Period Classification #1

p=15
x=Pts_List[[p]]
nc=levels$Mean[p]
fb=as.numeric(unlist(strsplit(ManualHierClasses$Breaks[p], split=", ")))
kmbrks <- classIntervals(x$Population.s2, style = "kmeans", n=nc)
x$km_breaks <-  cut(x$Population.s2, kmbrks$brks,labels = FALSE,include.lowest = TRUE)
Catch.sf = st_as_sf(Catch_List[[p]])

# MAP #1

ggp1 = ggplot() +  geom_stars(data = Hillshade.s)+
  scale_fill_gradientn(colours = c("turquoise", "black", "white"), 
               values = scales::rescale(c(-9999, -161, 254)), guide="none")+
  geom_sf(data = CatchLims.sf, color="black", size=2, alpha = 0) +
  #geom_sf(data = Catch.sf, color="black", size=0.3, alpha = 0) +
  ggnewscale::new_scale_fill()+
  geom_point(data=x@data, mapping=aes(x=East, y=North, fill=as.factor(km_breaks), size=Population.s2), shape=21, color = "black", stroke = 1, alpha=0.55)+
  guides(fill = guide_legend(order = 1, override.aes = list(size = 3,nrow=6)),
         size = guide_legend(order = 2, override.aes = list(nrow=3)))+
  labs(subtitle = "Period 15, EA (c.AD 1200-1350)", fill="Levels", size="Population")+
  scale_fill_brewer(palette = "Dark2")+
  coord_sf()+
  theme_void()+
  theme(legend.justification=c(0,1), legend.position=c(0.03,0.9), 
        legend.spacing.y = unit(0.1,"line"),legend.text=element_text(size=9, face="bold"),
        legend.title=element_text(size=12, face="bold"),
        legend.key = element_rect(colour = "transparent", fill = "white"), 
        legend.background = element_rect(colour = 'black', fill = 'white', linetype='solid'),
        plot.subtitle = element_text(hjust = 0.5, face="bold", size=16))

# Settlement Heirarchy Period Classification #2

p=5
x=Pts_List[[p]]
nc=levels$Mean[p]
kmbrks <- classIntervals(x$Population.s2, style = "kmeans", n=nc)
x$km_breaks <-  cut(x$Population.s2, kmbrks$brks,labels = FALSE,include.lowest = TRUE)
Catch.sf = st_as_sf(Catch_List[[p]])

# MAP #2

ggp2 = ggplot() +  geom_stars(data = Hillshade.s)+
  scale_fill_gradientn(colours = c("turquoise", "black", "white"), 
               values = scales::rescale(c(-9999, -161, 254)), guide="none")+
  geom_sf(data = CatchLims.sf, color="black", size=2, alpha = 0) +
  #geom_sf(data = Catch.sf, color="black", size=0.3, alpha = 0) +
  ggnewscale::new_scale_fill()+
  geom_point(data=x@data, mapping=aes(x=East, y=North, fill=as.factor(km_breaks), size=Population.s2), shape=21, color = "black", stroke = 1, alpha=0.55)+
  guides(fill = guide_legend(order = 1, override.aes = list(size = 3,nrow=3)),
         size = guide_legend(order = 2, override.aes = list(nrow=4)))+
  labs(subtitle = "Period 5, LF (c.400-200 BC)", fill="Levels", size="Population")+
  scale_fill_brewer(palette = "Dark2")+
  coord_sf()+
  theme_void()+
  theme(legend.justification=c(0,1), legend.position=c(0.03,0.9), 
        legend.spacing.y = unit(0.1,"line"),legend.text=element_text(size=9, face="bold"),
        legend.title=element_text(size=12, face="bold"),
        legend.key = element_rect(colour = "transparent", fill = "white"), 
        legend.background = element_rect(colour = 'black', fill = 'white', linetype='solid'),
        plot.subtitle = element_text(hjust = 0.5, face="bold", size=16))

#combine and arrange using cowplot package

plot_row <- plot_grid(ggp2, ggp1, align = "h", nrow = 1, rel_heights = c(1/2, 1/2))

title <- ggdraw() + draw_label("SBOM Kmeans Settlement Hierarchy Classes",fontface = 'bold',hjust = 0.5, size=20 )

out <- plot_grid(title, plot_row,ncol = 1, rel_heights = c(0.1,0.9)) + 
          theme(plot.background = element_rect(fill = "white", colour = NA))

#save figure

ggsave("KmeansSetHierMaps.png", plot = out, device = "png", path = wd$figs, scale = 1, width = 8, height = 4.3,   units = "in",  dpi = 1500)

rm(plot_row,title,ggp1,ggp2,p,x,nc,kmbrks,Catch.sf,out)

# import figure for display

knitr::include_graphics(paste0(wd$figs,"KmeansSetHierMaps.png"), FALSE)
```

### Classifying All Periods

Now we can classify all periods using this method

```{r, 'Calculate Hierarchy Classes for all Periods', message=FALSE,warning=FALSE}

for (i in 1:length(Poly_List)){
  
  nc=levels$Mean[i]
  
  kmbrks <- classIntervals(Poly_List[[i]]$Population.s2, style = "kmeans", n=nc)
  
  Poly_List[[i]]$SetHierLevel <-  cut(Poly_List[[i]]$Population.s2, 
                                      kmbrks$brks, labels = FALSE, 
                                      include.lowest = TRUE)
}

```




## Distances to 

--urban settlements
--higher hierarchical level (up and down)

spbb$distance <- rowMins(dist_matrix)              # get the dist of nearest element
spbb$nearest  <- rowMins(dist_matrix, value = T)   # get the index of the nearest element


Destination accessibility or centrality or competition 
--distance weighted local access to urban services
--dist weighted local access to economic opportunity AND/OR competition
--regional socioeconomic inertia
--core-periphery location

the potential accessibility of settlement $j$ to all $n$ destination settlements $i$
$$
A_{j}=\sum_{i=1}^{n} \left( \frac{W_{i}}{D_{ji}} \right)
$$
where $W_{i}$ is the weight of a given destination, and $D_{ji}$ is the distance between settlement $j$ and destination $i$.

this is core-periphery

--Among all settlements, with population as the weight, access to economic opportunity/demand/supply 

--Among all settlements, with UrbanPop/Urbanization as the weight, access to urban/sociopolitical services + institutions

W_ij as a relational term, dependent on similarity or difference of metrics

For certain variables/relationships, 
--similarity == competition
--difference == complimentarity
carrying capacity, pop pressurem ag potential, urbanization, urban pop
rural and urban goods circulation complimentarity; regional division of labor; specialization in the urban hierarchy
political-economic elites of larger centers competing for control; likewise smaller centers vying for an edge over their neighbors
ENvironmental differences == complimentary

Growth rates need to be standardized!!!! Perhaps across periods...?
--rescaled [0,1]

--
-- is 1-




## Relative Demographic Scale Metrics

--Specialization/concentration/inequality metrics on demography variables

Pop scale = = settlement pop / max settlement pop. Pop size with respect to max of whole region.

Pop Rank = Overall rank in settlement hierarchy

NO SPATIAL METRICS because the spatial element is going into the gravity model as distance decay!!!

Pop_PropMax.s2

Pop_PropMax.s2
Pop_Rank.s2
UrbanPop_PropMax.s2
UrbanPop_Rank.s2
r12_Pert_Rank.s2
PopDens_PropMax.s2
PopDens_Rank.s2
UrbanScale_PropMax.s2
UrbanScale_Rank.s2


```{r, 'Global Settlement Hierarchy Metrics', message=FALSE,warning=FALSE}

Poly_List2 <- list() #create output list

for(i in 1:length(Catch_List)){
  
  tmp.p <- Poly_List[[i]] #define site polys as temp object
  
  tmp.p@data$Pop_PropMax.s2 <- tmp.p@data$Population.s2/(max(tmp.p@data$Population.s2, na.rm=T))
  
  tmp.p@data$Pop_Rank.s2 <- rank(-tmp.p@data$Population.s2, na.last = "keep", ties.method = "average")
  
  if((max(tmp.p@data$UrbanPop.s2, na.rm=T)) > 0){
    tmp.p@data$UrbanPop_PropMax.s2 <- tmp.p@data$UrbanPop.s2/(max(tmp.p@data$UrbanPop.s2, na.rm=T))} else{tmp.p@data$UrbanPop_PropMax.s2 <- NA}
  
  if((max(tmp.p@data$UrbanPop.s2, na.rm=T)) > 0){
    tmp.p@data$UrbanPop_Rank.s2 <- rank(-tmp.p@data$UrbanPop.s2, na.last = "keep", ties.method = "average")} else {tmp.p@data$UrbanPop_Rank.s2 <- NA}
  
  tmp.p@data$r12_Pert_Rank.s2 <- rank(-tmp.p@data$r12_Pert.s2, na.last = "keep", ties.method = "average")
  
  tmp.p@data$PopDens_PropMax.s2 <- tmp.p@data$PopDens.s2/(max(tmp.p@data$PopDens.s2, na.rm=T))
  
  tmp.p@data$PopDens_Rank.s2 <- rank(-tmp.p@data$PopDens.s2, na.last = "keep", ties.method = "average")
  
  tmp.p@data$UrbanScale_PropMax.s2 <- tmp.p@data$UrbanScale.s2/(max(tmp.p@data$UrbanScale.s2, na.rm=T))
  
  tmp.p@data$UrbanScale_Rank.s2 <- rank(-tmp.p@data$UrbanScale.s2, na.last = "keep", ties.method = "average")
  
  tmp.p@data <- tmp.p@data %>% rowwise() %>% 
    mutate(
        r12_Pert_Rank.s2 = ifelse(is.na(r12_Pert_Rank.s2), max(r12_Pert_Rank.s2)+1, r12_Pert_Rank.s2),
        UrbanPop_Rank.s2 = ifelse(is.na(UrbanPop_Rank.s2), max(UrbanPop_Rank.s2)+1, UrbanPop_Rank.s2),
        Pop_Rank.s2 = ifelse(is.na(Pop_Rank.s2), max(Pop_Rank.s2)+1, Pop_Rank.s2),
        UrbanScale_Rank.s2 = ifelse(is.na(UrbanScale_Rank.s2), max(UrbanScale_Rank.s2)+1, UrbanScale_Rank.s2)) %>% ungroup()
  
  Poly_List2[[i]] <- tmp.p #save to output list
  
}
# rename list items
names(Poly_List2) <- sitepoly.names
Poly_List <- Poly_List2

rm(Poly_List2)
```



Spatial Gini 






# Local Settlement Hierarchy Variables

Local Spatial Interaction Proxies

Local variability/diversity
--sd
--variance

Standardized against global averages
--global average for a metric; value as a a fraction of the global average
--OR as a Z-score...

https://cran.rstudio.com/web/packages/tabula/vignettes/diversity.html

IN...
--Population.s2
--UrbanPop.s2
--UrbanPop_PropMax.s2
--r12_Pert_Rank.s2
--Catch_Popdens.s2
--Catch_ha
--UrbanScale.s2
--PctUrban.s2
--TranspDens.pct
--NPP.avg
--Elev.avg
--Slope.avg
--TPI.avg
--TRI.avg
--Rough.avg
--EZ.avg

lctools R PACKAGE
https://cran.r-project.org/web/packages/lctools/index.html
lctools::spGini = Spatial Gini coefficient
lctools::spGini.w = Spatial Gini coefficient with a given weights matrix
lctools::mc.spGini = Monte Carlo simulation for the significance of the Spatial Gini coefficient
lctools::acc = Spatial Interaction Models: Destination Accessibility

FLQ = Focal Location Quotient
gw_variable = Spatial Interaction Models: gw / regional variable
l.moransI = Local Moran’s I classic statistic for assessing spatial autocorrelation
lcorrel = Local Pearson and GW Pearson Correlation
mc.lcorrel = Monte Carlo simulation for the significance of the local correlation coefficients
w.matrix = Weights Matrix based on a number of nearest neighbours or a fixed distance
https://cran.r-project.org/web/packages/SpatialAcc/SpatialAcc.pdf
https://cran.r-project.org/web/packages/SpatialML/SpatialML.pdf


https://cran.r-project.org/web/packages/ggspatial/ggspatial.pdf


```{r}
#convert to sf class polygons
AZsf = st_as_sf(AZ[,2:4])
#calculate 3km average for all sites
pts_agg <- aggregate(AZsf, AZsf, FUN = mean, na.rm=T, join = function(x, y) st_is_within_distance(x, y, dist = 3000)) 

sd(x, na.rm = FALSE)

var(x, y = NULL, na.rm = FALSE, use)


```







#A Coef

```{r}
nam <- gsub('_SitePoly', "", sitepoly.names)
nam <- nam %>% str_replace("._", "") %>% str_replace("1", "")

RSA_df_list <- list()
RSA_plot_list <- list()

for (i in 1:length(Poly_List)){
  
  tmp <- RS_Acoef(z=Poly_List[[i]]$Population.s2, ids=Poly_List[[i]]$AggSite, plot_title = paste0(i,". ",nam[i]),yaxis_title = "Log Population")
  
  RSA_plot_list[[i]] <- tmp[[1]]
  
  x <- tmp[[2]]
  xx=as.data.frame(t(as.matrix(x$Value)))
  colnames(xx) = x$Metric
  xx$Period <- nam[i]
  xx$PeriodNum <- i
  RSA_df_list[[i]] <- xx
}

names(RSA_plot_list) <- nam
RSA_df = do.call(rbind, RSA_df_list)

rm(x,xx,tmp)

```















# Recombining and Reorganizing the Data

```{r, 'Recombining the Data Part 2', message=FALSE, warning=FALSE}
# Convert lists of period-wise sites/catchments to single SPDF objects
All_Agg_SitePoly <-  do.call(rbind, Poly_List)
All_Agg_CatchPoly <- do.call(rbind, Catch_List)

# variables from site data that needs transfer over to catchment areas
colz1 = setdiff(colnames(All_Agg_SitePoly@data),colnames(All_Agg_CatchPoly@data))
# variables from catchment areas that needs transfer over to site data
colz2 = setdiff(colnames(All_Agg_CatchPoly@data),colnames(All_Agg_SitePoly@data))

#reorder the data to match
All_Agg_SitePoly <- All_Agg_SitePoly[order(All_Agg_SitePoly$AggSite),]
All_Agg_CatchPoly <- All_Agg_CatchPoly[order(All_Agg_CatchPoly$AggSite),]

#check to see that the two datasets are in the right order
#identical(All_Agg_SitePoly@data$AggSite, All_Agg_CatchPoly@data$AggSite)

# Site data to catchment areas
Site_to_Catch <- All_Agg_SitePoly@data %>% select(!!!syms(colz1))
All_Agg_CatchPoly@data <- cbind(All_Agg_CatchPoly@data,Site_to_Catch)

# catchment area data to sites
Catch_to_Site <- All_Agg_CatchPoly@data %>% select(!!!syms(colz2))
All_Agg_SitePoly@data <- cbind(All_Agg_SitePoly@data,Catch_to_Site)

#Reorganize the data

ordering <- c(
  #ID VARIABLES
      "AggSite","AggID","Site","East","North","SurvReg","Number","CerPhase","Period", 
      "PeriodType","PeriodLength","PeriodNum","PeriodBegin","PeriodEnd", 
      "OccSeqLoc","OccSeqLoc.Sites","SubOccSeqLoc","SubOccSeqLoc.Sites",
      "ComponentNum", "ComponentSites",
  #CHRONOLOGICAL VARIABLES
      "PeriodInterval", "PeriodBegin", "PeriodBegin.era", "PeriodMidpoint", 
      "PeriodMidpoint.era", "PeriodEnd", "PeriodEnd.era", "PeriodLength",
  #OCCUPATION VARIABLES (COUNTS)
      "Occ.EF","Occ.EF_MF","Occ.MF","Occ.MF_LF","Occ.LF","Occ.LF_TF",
      "Occ.TF","Occ.TF_CL","Occ.CL","Occ.CL_ET","Occ.ET","Occ.ET_LTAzI", 
      "Occ.LTAzI","Occ.LTAzI_EA","Occ.EA","Occ.EA_LA","Occ.LA","Occ.TOT",
  #SUBOCCUPATION VARIABLES (COUNTS)
      "SubOcc.EF","SubOcc.EF_MF","SubOcc.MF","SubOcc.MF_LF","SubOcc.LF",
      "SubOcc.LF_TF","SubOcc.TF","SubOcc.TF_CL","SubOcc.CL","SubOcc.CL_ET",
      "SubOcc.ET","SubOcc.ET_LTAzI","SubOcc.LTAzI","SubOcc.LTAzI_EA",
      "SubOcc.EA","SubOcc.EA_LA","SubOcc.LA","SubOcc.TOT",
  #SITE AREA AND OCCUPATIONAL DENSITY VARS
      "Area_ha","Perim_m2","SherdDens","Tot.Assemb","FwOvlp.Assemb", 
      "BwOvlp.Assemb", "Net.Assemb",
  #STEP #2 DEMOGRAPHIC VARIABLES
      "Prior.s2", "Observed.s2", "MeanOccuProb.s2", "Population.s2",
      "Log_Population.s2", "ApportAssemb.s2", "r12_Pert.s2","PopDens.s2",
      "UrbanScale.s2", "UrbanPop.s2","RuralPop.s2", "PctUrban.s2","PctRural.s2",
  #STEP #1 DEMOGRAPHIC VARIABLES
      "Population.s1","PopDens.s1","UrbanScale.s1","UrbanPop.s1","RuralPop.s1", 
      "PctUrban.s1","PctRural.s1",
  #CONTINUITY VARIABLES
      "AreaBwCont","AreaFwCont","PopBwCont","PopFwCont","FwOvlp.Sites",
      "FwOvlp.Area","FwOvlp.Pop","BwOvlp.Sites","BwOvlp.Area","BwOvlp.Pop",
  #PERSISTENCE VARIABLES
      "Found","FoundInit","Abandon","Persist","DewarType","OccuIntertia",
  #STEP #3 GLOBAL SETTLEMENT HIERARCHY VARIABLES
      "Pop_PropMax.s2", "Pop_Rank.s2", "UrbanPop_PropMax.s2", "UrbanPop_Rank.s2", 
      "r12_Pert_Rank.s2", "PopDens_PropMax.s2", "PopDens_Rank.s2", 
      "UrbanScale_PropMax.s2", "UrbanScale_Rank.s2", 
  #STEP #3 CATCHMENT AREA AND POP DENSITY VARIABLES
      "Catch_ha", "CatchB_ha", "Catch_Popdens.s2", 
      "Catch_Popdens_PropMax.s2","Catch_Popdens_Rank.s2","CatchB_Popdens.s2",
      "CatchB_Popdens_PropMax.s2", "CatchB_Popdens_Rank.s2", 
  #STEP #3 TRANSPORT VARIABLES
      "TranspDens.pct", "TranspDens.rank",##,core-periphery
  #STEP #3 CATCHMENT ENVIRONMENT/TOPOGRAPHY VARIABLES
      "NPP.tot","NPP.avg","EZ.avg","EZ.sd","TRI.tot","TRI.avg","IrrigPot.tot",
      "IrrigPot.avg","WetAgPot.tot","WetAgPot.avg","IntnsCost.tot",
      "IntnsCost.avg","IntnsCostNW.tot","IntnsCostNW.avg","AGPot.tot",
      "AGPot.avg","AGPotNW.tot","AGPotNW.avg","PopPressure","PopPressureNW",
      "ErosionPot.tot","ErosionPot.avg", 
  #SURVEY METADATA
      "M_Sites","M_SiteCode","M_SiteName","M_FieldSite.Region",
      "M_FieldSite.Period","M_SurveyYearNumber","M_Supervisor","M_Map",
  #OLD tDAR BOM SURVEY VARIABLES
      "O_Elev","O_ElevMed","O_ElevMin","O_ElevMax","O_EZcode",
      "O_EnvironmentalZone","O_Soil","O_SoilMed","O_SoilMin","O_SoilMax",
      "O_Erosion","O_ErosionMed","O_ErosionMin","O_ErosionMax","O_ModernUse",
      "O_ModernSettlement","O_Rainfall","O_Area","O_MoundDomestic",
      "O_MoundCeremonial","O_MoundQuestionable","O_MoundTotal",
      "O_MoundRecorded","O_DMoundArea","O_Architecture","O_TerraceConfidence",
      "O_TerraceExtent","O_Sherd","O_SherdMed","O_SherdMin","O_SherdMax",
      "O_Rubble","O_RubbleMed","O_RubbleMin","O_RubbleMax","O_Population",
      "O_PopMin","O_PopMax","O_PopMethod","O_stcode","O_SiteType",
      "O_SubPeriod1","O_SubPeriod2","O_OccEF","O_OccMF","O_OccLF","O_OccTF",
      "O_OccCL","O_OccEC","O_OccMC","O_OccLC","O_OccET","O_OccLT","O_OccAZ",
      "O_OccEA","O_OccLA","O_OccTot","O_OccSeqLoc","O_SubOc1","O_SubOc2",
      "O_PdDupSite","O_Group","O_Comments") 

#Make sure everything is kosher
#setdiff(colnames(All_Agg_CatchPoly@data),ordering)
#setdiff(colnames(All_Agg_SitePoly@data),ordering)
#setdiff(ordering,colnames(All_Agg_CatchPoly@data))
#setdiff(ordering,colnames(All_Agg_SitePoly@data))

# Reorder the data for both sites and catchment areas
All_Agg_SitePoly@data <- All_Agg_SitePoly@data %>% select(!!!syms(ordering))
All_Agg_CatchPoly@data <- All_Agg_CatchPoly@data %>% select(!!!syms(ordering))
```


# Export Data for Script #5

```{r, 'Export Data for Step #4', message=FALSE,warning=FALSE}

#AggSite polygons
writeOGR(All_Agg_SitePoly, paste0(dir2,"SBOM_Agg_SitePoly3.gpkg"), "SBOM_Agg_SitePoly3", driver = "GPKG", overwrite_layer=TRUE)

#Catchment areas
writeOGR(All_Agg_CatchPoly, paste0(dir2,"SBOM_Agg_CatchPoly3.gpkg"), "SBOM_Agg_CatchPoly3", driver = "GPKG", overwrite_layer=TRUE)

#Catchment Limits
writeOGR(CatchLims, paste0(dir2,"CatchLims.gpkg"), "CatchLims", driver = "GPKG", overwrite_layer=TRUE)
```


# References

